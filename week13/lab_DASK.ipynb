{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"IMG/dask_horizontal.svg\"  width=\"30%\">\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to DASK\n",
    "Dask is a parallel computing library that scales the existing Python ecosystem. This tutorial will introduce Dask and parallel data analysis more generally.\n",
    "\n",
    "Dask can scale down to your laptop laptop and up to a cluster.\n",
    "\n",
    "\n",
    "*  **High level Interface:**  Dask provides high-level Array, Bag, and DataFrame\n",
    "   collections that mimic NumPy, lists, and Pandas but can operate in parallel on\n",
    "   datasets that don't fit into memory.  Dask's high-level collections are\n",
    "   alternatives to NumPy and Pandas for large datasets.\n",
    "\n",
    "*  **Low Level Interface:** Dask provides dynamic task schedulers that\n",
    "   execute task graphs in parallel.  These execution engines power the\n",
    "   high-level collections mentioned above but can also power custom,\n",
    "   user-defined workloads.  These schedulers are low-latency (around 1ms) and\n",
    "   work hard to run computations in a small memory footprint.  Dask's\n",
    "   schedulers are an alternative to direct use of `threading` or\n",
    "   `multiprocessing` libraries in complex cases or other task scheduling\n",
    "   systems like `Luigi` or `IPython parallel`.\n",
    "   \n",
    "## DASK Ressources \n",
    "*  [DASK Website](http://dask.org)\n",
    "*  [Documentation](https://dask.pydata.org/en/latest/)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%run prep.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# High Level Interfaces: Parallelizing NumPy and Pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pandas\n",
    "Pandas Dataframes are a very popular tool that provide full speadsheet and in-memory database functionality for the ***Python*** ecco system. It has become one of tstandard tools in Data Science and is also widely used for pre-processing / data wrangling tasks in machine learning\n",
    "\n",
    "* the ***DASK*** API for ***Pandas*** is analog to the one for ***NumPy*** and will not be discussed here, see https://docs.dask.org/en/latest/dataframe.html for details.\n",
    "\n",
    "## NumPy\n",
    "As seen before, NumPy arrays are the dominat array and matrix data structure in the ***Python*** data science and machine learning ecco system.\n",
    "* refresh your NumPy skills with this extra [tutorial](Extra_Material/Numpy.ipynb)\n",
    "\n",
    "### Functionality of Dask arrays\n",
    "\n",
    "* Arithmetic and scalar mathematics: +, *, exp, log, ...\n",
    "* Reductions along axes: sum(), mean(), std(), sum(axis=0), ...\n",
    "* Tensor contractions / dot products / matrix multiply: tensordot\n",
    "* Axis reordering / transpose: transpose\n",
    "* Slicing: x[:100, 500:100:-2]\n",
    "* Fancy indexing along single axes with lists or NumPy arrays: x[:, [10, 1, 5]]\n",
    "* Array protocols like __array__ and __array_ufunc__\n",
    "* Some linear algebra: svd, qr, solve, solve_triangular, lstsq\n",
    "\n",
    "**Full Array API: https://docs.dask.org/en/latest/array-api.html**\n",
    "\n",
    "### Limitations\n",
    "However, Dask Array does not implement the entire NumPy interface. Users expecting this will be disappointed. Notably, Dask Array lacks the following features:\n",
    "\n",
    "* Much of np.linalg has not been implemented. This has been done by a number of excellent BLAS/LAPACK implementations, and is the focus of numerous ongoing academic research projects\n",
    "* Arrays with unknown shapes do not support all operations\n",
    "* Operations like sort which are notoriously difficult to do in parallel, and are of somewhat diminished value on very large data (you rarely actually need a full sort). Often we include parallel-friendly alternatives like topk\n",
    "* Dask Array doesnâ€™t implement operations like tolist that would be very inefficient for larger datasets. Likewise, it is very inefficient to iterate over a Dask array with for loops\n",
    "  \n",
    "**DASK is still evolving ...**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Just get started!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#let's parallelize a simple array opperations\n",
    "import numpy as np\n",
    "np_A = np.random.rand(10000,10000) #create random NumPy array\n",
    "%time np_A.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#now in parallel \n",
    "import dask.array as da\n",
    "da_A = da.from_array(np_A, chunks=(5000)) #create DASk array from existing NumPy array\n",
    "res = da_A.sum() #tell dask to compute the sum of A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Where is my result?\n",
    "In a nutshell: ***DASK*** basically works like this:\n",
    "* First, it builds an execution graph\n",
    "* Then, we need to execute it by calling ``compute()``"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#call compute\n",
    "%time res.compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why is this not faster?\n",
    "Well, building the graph, parallelizing it and scheduling it's execution causes some overhead...\n",
    "* probably, our problem was to small :-)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is happening at ``compute()`` ?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#need one mor lib\n",
    "!conda install -y python-graphviz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#have look at the actual compute graph build by DASK\n",
    "res.visualize(filename='graph.svg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ahh - MapReduce!\n",
    "* looks like ***DASK*** automatically implemented MapReduce\n",
    "* but, why did it use 4 parallel paths?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Block concept of DASK arrays\n",
    "* ***DASK*** splitts arrays into blocks \n",
    "* parallelization (here MapReduce) runs over Blocks\n",
    "* Blocks sizes are definde by the ``chunks`` \n",
    "\n",
    "```\n",
    "#recall array creation\n",
    "da_A = da.from_array(np_A, chunks=(5000))\n",
    "```\n",
    "\n",
    "<img SRC=\"IMG/array.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get size of chucks\n",
    "da_A.chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "da_A.chunksize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#change chunk size\n",
    "#WARNING: can be very expensive - better think about the chunk size before ...\n",
    "B=da_A.rechunk(1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "B.chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res=B.sum()\n",
    "graph=res.visualize(filename='graph.svg')#saving graph to file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Is this running in Parallel ?\n",
    "* open ternminal\n",
    "* start ``top``\n",
    "* execute ``compute()``"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit #measure mean of 10 runs\n",
    "res.compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Memory Usage\n",
    "By processing each block at a time, ***DASK*** is not only able tu run in parallel, but also needs only tthe memory for the bock size...\n",
    "* allows easy processing of **larger than memory** data sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#a larger example in NumPy\n",
    "%time x = np.random.normal(10, 0.1, size=(20000, 20000)) \n",
    "%time y = x.mean(axis=0) #compute mean along first axis\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#same in DASK\n",
    "x = da.random.normal(10, 0.1, size=(20000, 20000), chunks=(1000, 1000))\n",
    "y = x.mean(axis=0) \n",
    "%time res=y.compute() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Distributed Execution\n",
    "## NOTE: the rest of this notebook requires a running DASK Cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask.distributed import Client #import distributed client"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In our cluster enviroment, the ***DASK*** Cluster is automatically started for multi node jobs. See DASK documentation on how to setup a ***DASK*** cluster: http://distributed.dask.org/en/latest/setup.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = Client() #start local cluster on CPU cores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#see how many workers we have\n",
    "workers=client.scheduler_info()['workers']\n",
    "for w in workers:\n",
    "    print(w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vist [http://localhost:8787](http://localhost:8787) to see your cluster status"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submit work to the Cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#simple remote functions\n",
    "def square(x):\n",
    "    return x ** 2\n",
    "\n",
    "def neg(x):\n",
    "    return -x\n",
    "\n",
    "#using the map future, functions will be scheduled for execution - NO graph building and compute()\n",
    "A = client.map(square, range(1000))\n",
    "B = client.map(neg, A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total = client.submit(sum, B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "res = total.result()\n",
    "print (\"done: \",res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Arrays on the Cluster\n",
    "Starting a cluster client automatically cause the high level interfaces to use the cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np_A = np.random.rand(10000,10000) #create random NumPy array\n",
    "import dask.array as da\n",
    "cluster_A = da.from_array(np_A, chunks=(5000)) #create DASk array from existing NumPy array\n",
    "res = cluster_A.sum() #tell dask to compute the sum of A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "%time res.compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ***DASK-ML*** Project\n",
    "Brand new approach to make ***DASK*** a full distributed ML framework.\n",
    "http://dask-ml.readthedocs.io/en/latest/joblib.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#example: Clustering\n",
    "%matplotlib inline\n",
    "\n",
    "import dask_ml.datasets\n",
    "import dask_ml.cluster\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#generate like in execise 2, but now with dask_mL\n",
    "X, y = dask_ml.datasets.make_blobs(n_samples=10000000,\n",
    "                                   chunks=1000000,\n",
    "                                   random_state=0,\n",
    "                                   centers=3)\n",
    "X = X.persist() #manual executions on X\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#call skleran k-means\n",
    "km = dask_ml.cluster.KMeans(n_clusters=3, init_max_iter=2000, oversampling_factor=10)\n",
    "km.fit(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(X[::10000, 0], X[::10000, 1], marker='.', c=km.labels_[::10000],  cmap='viridis', alpha=0.25);\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Low Level Interfaces: Parallelizing Compute Graphs\n",
    "Building the Compute-Graph manually"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# simple example\n",
    "def inc(x):\n",
    "    return x + 1\n",
    "\n",
    "def double(x):\n",
    "    return x + 2\n",
    "\n",
    "def add(x, y):\n",
    "    return x + y\n",
    "\n",
    "data = [1, 2, 3, 4, 5]\n",
    "\n",
    "output = []\n",
    "for x in data:\n",
    "    a = inc(x)\n",
    "    b = double(x)\n",
    "    c = add(a, b)\n",
    "    output.append(c)\n",
    "\n",
    "total = sum(output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Now, build a graph:**\n",
    "```dask.delayed()``` add functions and arguments to graph:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask\n",
    "\n",
    "output = []\n",
    "for x in data:\n",
    "    a = dask.delayed(inc)(x)\n",
    "    b = dask.delayed(double)(x)\n",
    "    c = dask.delayed(add)(a, b)\n",
    "    output.append(c)\n",
    "\n",
    "total = dask.delayed(sum)(output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total.visualize(filename=\"graph2.svg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#call copute to execute\n",
    "total.compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
